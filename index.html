<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <title>syosa: 非言語情報の抽出・可視化ツール</title>
    <meta name="keywords" content="ページのキーワードA,B,C">
    <meta name="description" content="人物による活動や発言が含まれる映像資料から登場人物の非言語情報を可視化するツールのページ">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="stylesheet.css">
</head>

<body>
    <div id="outer">
        <header>
            <h1 id="tool-title">syosa: 非言語情報の抽出・可視化ツール</h1>
            <p>人物による活動や発言が含まれる映像資料から登場人物の非言語情報を可視化するツール</p>
        </header>
        <div id="content">
            <div class="inner">
                <h1 id="tool-info">ツールについて</h1>
                <a href="https://github.com/t0tanikawa/syosa/tree/main">実行ファイルはこちら</a>
                <h2 id="tool-summary">概要</h2>
                <p>本ツール(syosa)は，人物による活動や発言が含まれる映像資料から，機械学習を用いた動画解析により登場人物の本人の表情や視線，瞬き，動作といった非言語情報の抽出を行い，音声認識による言語情報ともにタイムラインデータとして保存可能とする．
                </p>
                <p>本ツールはWebサーバー上に実行可能な形で置くことで，Webページの形で利用可能となる．利用者がページにアクセスすると映像資料のファイルを指定することで登場人物の非言語情報の解析が行われる．読み込まれたデータはサーバーにアップロードはされず，利用者がアクセスしている計算機資源を使って処理が行われ，いかなる利用履歴やデータ収集も行われない．
                </p>
                <img src="syosa-process.png" alt="解析処理画面" style="width:100%;">


                <h2 id="tool-result">解析結果</h2>
                <p>非言語情報を抽出するため，映像から以下の通りの情報を取得することが可能になっている．顔の表情は登場人物の感情を反映しているため極めて重要である．首の動きや瞳の方向をトレースすることで，相手への関心の高さや心理状態などを得られる可能性がある．また手の動きや体といった身振り手振りによる情報は非言語コミュニケーションの重要な要素である．口の開口率については感情や声の強さなどを表し伝えたい内容の理解において有用である．その他，年齢や性別，人物同定はコミュニケーションの理解を支援するために行った．
                </p>

                <ul>
                    <li>身体および顔のランドマークの座標</li>
                    <li>顔の姿勢</li>
                    <li>顔、身体、手のジェスチャー（状態）</li>
                    <li>瞳の方向</li>
                    <li>口の開口率</li>
                    <li>性別推定</li>
                    <li>年齢推定</li>
                    <li>人物同定</li>
                    <li>物体（オブジェクト認識）</li>
                    <li>表情</li>
                </ul>
                <img src="syosa-visualization.png" alt="解析結果のビジュアライゼーション" style="width:100%;">


                <p>
                    また，認識した結果をタイムグラフに合わせて表示でき，必要に応じて可視化内容は変更できる．表情は，基本的な6つの感情（怒り・嫌悪・恐怖・喜び・悲しみ・驚き）の比率で表示される．横軸のタイムスケールは拡大縮小可能である．身体情報としては，頷きなどに関連する頭部の姿勢情報の可視化される．ジェスチャーについても認識したものをタイムライン上に表示される．
                </p>
                <p>言語情報については，時間指定のテキストトラック（字幕やキャプションなど）を表示するためのウェブビデオテキストトラック形式 (WebVTT) を読み込むことで対応する．</p>

                <h2 id="tool-acknowledgment">謝辞</h2>
                <p>本ツールは，<a href="https://www.nihu.jp/"
                        target="_blank">大学共同利用機関法人人間文化研究機構</a>の2023年度デジタル・ヒューマニティーズ(DH)研究等補助事業の支援を受けて開発された．</p>

                <h2 id="tool-license">ライセンス</h2>
                <p>プログラムおよび学習データについて以下のライセンス形態により個人利用および非商用利用において公開・使用・再配布を行う</p>
                <p><small>
                        Human Library: MIT License<br>
                        SSR-Net: Apache License Version 2<br>
                        Real-of-Fake: 未使用（Face Anti-Spoofing）<br>
                        BlazePose: Apache License Version 2<br>
                        EfficientPose: Apache License Version 2<br>
                        MoveNet: Apache License Version 2(TensorFlow)<br>
                        PoseNet: Apache License Version 2(TensorFlow)<br>
                        MediaPipe Meet: Google Terms of Service<br>
                        MediaPipe Selfie: Apache License Version 2<br>
                        Robust Video Matting: GNU General Public License v3.0<br>
                        Oarriaga: MIT License<br>
                        HSE-AffectNet: Apache License Version 2<br>
                        MediaPipe Iris: Apache License Version 2<br>
                        HSE-FaceRes: Apache License Version 2<br>
                        MediaPipe BlazeFace: Apache License Version 2<br>
                        BecauseofAI MobileFace: MIT License<br>
                        DeepInsight InsightFace: MIT License<br>
                        MediaPipe FaceMesh: Apache License Version 2<br>
                        MediaPipe FaceMesh Attention Variation: Apache License Version 2<br>
                        GEAR Predictor: GNU General Public License v3.0<br>
                        MediaPipe HandPose: Apache License Version 2<br>
                        HandTracking: MIT License<br>
                        WebGLImageFilter: MIT License<br>
                        MB3-CenterNet: 未使用（Object Detection）<br>
                        NanoDet: Apache License Version 2<br>
                        Pinto: MIT License<br>
                        CanvasJS: 個人利用・非商用利用は無償（クレジットおよびリンク必須）<br>
                        Bootstrap: MIT License<br>
                    </small></p>

            </div>
        </div>
        <aside>
            <div class="right-title">syosa</div>
            <div class="link">
                <ul>
                    <li><a href="#tool-title">ツールについて</a></li>
                    <li><a href="#tool-summary">概要</a></li>
                    <li><a href="#tool-result">解析結果</a></li>
                    <li><a href="#tool-acknowledgment">謝辞</a></li>
                    <li><a href="#tool-license">ライセンス</a></li>
                </ul>
            </div>
        </aside>
        <footer>t0tanikawa</footer>
    </div>
</body>

</html>

</html>